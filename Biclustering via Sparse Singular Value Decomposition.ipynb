{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biclustering via Sparse Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jingxuan Zhang, Jae Hyun Lee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biclustering via Sparse Singular Value Decomposition(SSVD) is initially proposed by Lee et al.(2010). The main idea is that SSVD can be used as one of Biclustering methods in that it can identifies latent association in columns and row simultaneously in the course of singular value decomposition with sparsity limitation. Biclustering is a collection of unsupervised learning methods which detects checkboard structured association between rows and columns in data and is often used in high dimensional data. In this paper, we translated Lee et al's publically available R and matlab code about SSVD into python code and optimize their algorithm by numba with parallelization option to make the speed of execution even faster. After optimization, this algorithm is applied to both simulated data and real data. The simulated data is lung cancer data which is used in Lee's paper and real data is breast cancer data. At last, in the course of application, The performance of optimized SSVD will be compared with existing SSVD algorithm in their time and accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KEY WORDS: Biclustering, high dimensional data, singular value decomposition, sparsity, and optimazation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper Biclustering via Sparse Singular Value Decomposition' newly propose SSVD as a biclustering method. Biclustering is a collection of unsupervised learning methods which play a important role in exploratory data analysis, especially in high dimensional data. Biclustering identifies association between rows and columns in data simultaneously and produce latent checkboard structured in data which is more interpretable than raw data. This unsupervised method is widely used for High dimensional low sample size(HDLSS) data which cannot use traditional statistical methods that requires relatively large sample size. \n",
    "\n",
    "Recently, HDLSS is a common feature in medical data such as microarray gene expression and in text mining/categorization. SSVD can be applied to deal with these problems as one of biclustering methods. For application in text mining, Biclustering can be used to reveal association between documents and word clusters. In this paper, in the context of microarray gene analysis, Biclustering detect association between disease and certain gene expression in a lung cancer microarray dataset.\n",
    "\n",
    "According to Lee, SSVD naturally implement biclustering in the course of low rank approximation using singluar value decomposition with sparsity-inducing penalties. Non-zero entries in both sides' singular vectors are clustered and this makes SSVD can be used as biclustering method. By detecting non-zero clusters in data, it is possible to find association between certain rows and columns. However, regression with adaptive lasso penalty which does not have closed form of solution is used to decomposing sparsed matrix. Therefore, it requires iterative steps to find optimal value of sparsed matrix and this process takes most of time used in SSVD. Consequently, optimazation of SSVD in this paper focused on reducing computation cost and time of this iterative step. \n",
    "\n",
    "At last, using simulated lung cancer data and real world breast cancer data, comparison between optimized SSVD and existing SSVD algorithm will be conducted. Specifically, SSVD of first layer approximation which approximate original data with rank1 matrix will be conducted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripition of algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As decribed above, Biclustering methods are used to identify distinctive patterns in a large data matrices that are significantly associated between columns or rows. To realize this function, we uses a new algorithm proposed by Lee et al. called SSVD aiming to approximate of X in a low-rank version. This means that the SVD result of $u_k$ and $v_k$the will contain numbers of zeros, which is called sparcity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the objective function to minimize is:\n",
    "$∥X − suv^T ∥^2_F + \\lambda_u P_1(su) + \\lambda_v P_2(sv)$\n",
    "where $P_1(su)$ and $P_2(sv)$ are sparsity-inducing penalty terms whose forms will be given later, and $\\lambda_u$ and $\\lambda_v$ are two nonnegative penalty parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the knowledege of lasso regression, for a fixed $u$, $min ∥X − suv^T ∥^2_F + \\lambda_u P_1(su) + \\lambda_v P_2(sv) = min ∥X−u\\tilde{v}^T ∥^2_F + \\lambda_v P_2(\\tilde{v}) = min ∥Y−(I_d ⊗u)\\tilde{v}∥^2 +\\lambda_v P_2(\\tilde{v})$ \n",
    "where $\\tilde{v} = sv$, $Y = (x^T_1 ,...,x^T_d )^T \\in R^{n\\times d}$ with $x_j$ being the jth column of X, and ⊗ being the Kronecker product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up the penalty parameters with weight vectors, then $P_2(sv) = s \\sum_{i=1}^n w_{2,j} |v_j|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, for a fixed $v$, we can get a symmetric result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An Iterative Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the adaptive lasso penalty, the minimizing objective for SSVD can be rewritten as\n",
    "$min ∥X − suv^T ∥^2_F + \\lambda_u P_1(su) + \\lambda_v P_2(sv) = min ∥X−suv^T ∥_F +s \\lambda_u \\sum_{i=1}^n w_{1,i}|u_i|+s\\lambda_v \\sum_{j=1}^d w_{2,j}|v_j|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fixed $u$, the alternative minimizing objective can become $min ∥X−u\\tilde{v}T∥^2_F + \\lambda_v \\sum_{j=1}^d w_{2,j}|\\tilde{v}_j|$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the alternating for minimization between $u$ and $v$ can be iterated until convergence. After convergence, simply set $s = u^T Xv$ and the SSVD layer is given by $suv^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penalty Parameter Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The degrees of sparsity of u and v are closely related to the two penalty parameters $\\lambda_u$ and $\\lambda_v$. It is also proved that the degree of sparsity of $u$ or $v$ is a step function of the penalty parameter $\\lambda_u$ and $\\lambda_v$ respectively, selecting the parameters is equivalent to selecting the degrees of sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the penalized regression with fixed $u$, define\n",
    "$BIC(\\lambda_v)= \\frac{∥Y − \\hat{Y}∥^2}{nd \\cdot \\hat{\\sigma}^2} + \\frac{\\log(nd)}{nd} \\hat{df}(\\lambda_v)$ \n",
    "where $df(\\lambda_v)$ is the degree of freedom of $v$ and $\\sigma^2$ is the OLS estimate of the error variance from the model for fixed $u$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline for SSVD Algorithm\n",
    "\n",
    "Step 1. Apply the standard SVD to a data matrix $X$. Let $\\{s_{old}, v_{old}, u_{old}\\}$ denote the first SVD triplet.\n",
    "\n",
    "Step 2. Update the previous result:\n",
    "\n",
    "(a) Set $\\tilde{v_j} =sign\\{(X^T u_{old})_j\\}(|(X^T u_{old} )_j | − \\lambda_v w_{2,j} / 2)_+$, $j = 1,...,d$, where $\\lambda_v$ is the minimizer of $BIC(\\lambda_v)$ defined above. Let $\\tilde{v} = (\\tilde{v}_1,...,\\tilde{v}_d)^T$, $s = ∥\\tilde{v}∥$, and $v_{new} = \\tilde{v} / s$.\n",
    "\n",
    "(b) Set $\\tilde{u_i} =sign\\{(X^T v_{new})_i\\} (|(X^T v_{new} )_i | − \\lambda_u w_{1,i} / 2)_+$, $i = 1,...,d$, where $\\lambda_u$ is the minimizer of $BIC(\\lambda_u)$ defined above. Let $\\tilde{u} = (\\tilde{u}_1,...,\\tilde{u}_d)^T$, $s = ∥\\tilde{u}∥$, and $u_{new} = \\tilde{u} / s$.\n",
    "\n",
    "(c) Set $u_{old} = u_{new}$ and repeat Steps 2(a) and 2(b) until convergence.\n",
    "\n",
    "Step 3. Set $u = u_{new}$, $v = v_{new}$, $s = u^T_{new} X v_{new}$ at convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Penalized Sum-of-Squares Criterion\n",
    "According to the preliminary knowledge, the first SVD layer $s_1$, $u_1$, $v^T_1$, can form the best rank-one matrix approximation of X under the Frobenius norm, i.e., $(s_1, u_1, v_1) = argmin_{\\{s, u, v\\}} ∥X − suv^T ∥^2_F $\n",
    "where $s$ is a positive scalar, $u$ is a unit n-vector, and $v$ is a unit d-vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of optimazation for algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, optimazation and profiling of SSVD from Lee's publically available code is conducted using lung cancer data which was also used in Lee's paper. It is shown that lung cancer data has a lot of columns and small number of observations(56 $\\times$ 12625) which indicates that this data is HDLSS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "#!pip install line_profiler \n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timeit\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "from numba import jit\n",
    "from numba import int32, int64, float32, float64, boolean\n",
    "from numba import njit, prange\n",
    "from scipy import sparse as sp\n",
    "from sparsesvd import sparsesvd \n",
    "from scipy.sparse import csc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 12625)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data import\n",
    "X = np.loadtxt('data.txt').T\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla SSVD - unoptimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Threshold function\n",
    "def thresh(X,ttyp, plambda):\n",
    "    #ttyp: threshhold type\n",
    "    #plambda: penalty lambda\n",
    "    a = 3.7    \n",
    "    if ttyp==1:\n",
    "        tmp= np.sign(X)*(np.abs(X)>=plambda)\n",
    "        y = tmp*np.abs(X)-plambda\n",
    "    elif ttyp==2:\n",
    "        y = X*(np.abs(X)>paralambda)\n",
    "    return y\n",
    "\n",
    "#ssvd fucntion - unoptimized\n",
    "def ssvd_vanilla(X,param=None):\n",
    "    n, d = X.shape\n",
    "    ttypu = 1;ttypv = 1 #threshold type for u,v\n",
    "    gamu = 0;gamv = 0   #nonnegative parameter \n",
    "    u, s, v = np.linalg.svd(X)\n",
    "    u0 = u[:,0]\n",
    "    v = v.T; v0 = v[:,0]\n",
    "    tol = 10**-4\n",
    "    niter = 20\n",
    "    ud = 1\n",
    "    vd = 1\n",
    "    iters = 0\n",
    "    SST = np.sum(X**2)\n",
    "    while (ud > tol or vd > tol):\n",
    "        iters = iters + 1\n",
    "        \n",
    "        #Updating v\n",
    "        zv = np.matmul(X.T,u0)\n",
    "        wv = abs(zv)**gamv     #data driven weight\n",
    "        sigsq = (SST - np.sum(zv**2))/(n*d-d)\n",
    "        tv = np.sort(np.append(abs(zv**wv),0))\n",
    "        pv = sum(tv>0)\n",
    "        Bv = np.ones((d+1,1))*np.Inf\n",
    "        for i in range(0,pv):\n",
    "            lvc = tv[d-i]\n",
    "            para = {'ttyp': ttypv, 'plambda': lvc/wv[wv!=0]}\n",
    "            temp2 = thresh(zv[wv!=0],para['ttyp'],para['plambda'])\n",
    "            vc = temp2\n",
    "            Bv[i] = sum(sum((X - u0[:,np.newaxis]*vc[:,np.newaxis].T)**2)/sigsq + i*np.log(n*d))\n",
    "        Iv = np.argmin(Bv)+1\n",
    "        temp = np.sort(np.append(abs(zv* wv),0))\n",
    "        lv = temp[d-Iv-1]\n",
    "        para['plambda'] = np.multiply(lv, wv[wv!=0])\n",
    "        temp2 = thresh(zv[wv!=0],para['ttyp'],para['plambda'])\n",
    "        v1 = temp2\n",
    "        v1 = v1/np.sqrt(sum(v1**2)) #v_new\n",
    "\n",
    "        #Updating u\n",
    "        zu = np.matmul(X, v1)\n",
    "        wu = abs(zu)**gamu\n",
    "        sigsq = (SST - sum(zu**2))/(n*d-n)\n",
    "        tu = np.sort(np.append(abs(zu**wu),0))\n",
    "        pu = sum(tu>0)\n",
    "        Bu = np.ones((n+1,1))*np.Inf\n",
    "        for i in range(0,pu):\n",
    "            luc = tu[n-i]\n",
    "            para = {'ttyp': ttypu, 'plambda': luc/wu[wu!=0]}\n",
    "            temp2 = thresh(zu[wu!=0],para['ttyp'],para['plambda'])\n",
    "            uc = temp2\n",
    "            Bu[i] = sum(sum((X - uc[:,np.newaxis]*v1[:,np.newaxis].T)**2)/sigsq + i*np.log(n*d))\n",
    "        Iu = np.argmin(Bu)+1\n",
    "        temp = np.sort(np.append(abs(zu*wu),0))\n",
    "        lu = temp[n-Iv-1]\n",
    "        para['plambda'] = lu/wu[wu!=0]\n",
    "        temp2 = thresh(zu[wu!=0],para['ttyp'],para['plambda'])\n",
    "        u1 = temp2\n",
    "        u1 = u1/np.sqrt(sum(u1**2)) #u_new\n",
    "\n",
    "        ud = np.sqrt(np.sum((u0-u1)**2))\n",
    "        vd = np.sqrt(np.sum((v0-v1)**2))\n",
    "        if iters > niter:\n",
    "            print('Fail to converge! Increase the niter!')\n",
    "            break\n",
    "        u0 = u1\n",
    "        v0 = v1\n",
    "    u = u1\n",
    "    v = v1\n",
    "    return u,v,iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29min 8s, sys: 1min 26s, total: 30min 34s\n",
      "Wall time: 27min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "u,v,iters = ssvd_vanilla(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 328 ms, sys: 764 ms, total: 1.09 s\n",
      "Wall time: 79 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "us,vs,iterss = ssvd_vanilla(X_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Profile printout saved to text file 'ssvd_results.txt'. \n",
      "cat: ssvd_vanilla.txt: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%lprun -s -f ssvd_vanilla -T ssvd_results.txt ssvd_vanilla(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\r\n",
      "\r\n",
      "Total time: 1677.03 s\r\n",
      "File: <ipython-input-3-b57477094a4e>\r\n",
      "Function: ssvd_vanilla at line 14\r\n",
      "\r\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\r\n",
      "==============================================================\r\n",
      "    14                                           def ssvd_vanilla(X,param=None):\r\n",
      "    15         1         16.0     16.0      0.0      n, d = X.shape\r\n",
      "    16         1          2.0      2.0      0.0      ttypu = 1;ttypv = 1 #threshold type for u,v\r\n",
      "    17         1          2.0      2.0      0.0      gamu = 0;gamv = 0   #nonnegative parameter \r\n",
      "    18         1   46887298.0 46887298.0      2.8      u, s, v = np.linalg.svd(X)\r\n",
      "    19         1         17.0     17.0      0.0      u0 = u[:,0]\r\n",
      "    20         1         11.0     11.0      0.0      v = v.T; v0 = v[:,1]\r\n",
      "    21         1          6.0      6.0      0.0      tol = 10**-4\r\n",
      "    22         1          2.0      2.0      0.0      niter = 20\r\n",
      "    23         1          2.0      2.0      0.0      ud = 1\r\n",
      "    24         1          2.0      2.0      0.0      vd = 1\r\n",
      "    25         1          2.0      2.0      0.0      iters = 0\r\n",
      "    26         1       3950.0   3950.0      0.0      SST = np.sum(X**2)\r\n",
      "    27        18         61.0      3.4      0.0      while (ud > tol or vd > tol):\r\n",
      "    28        17         33.0      1.9      0.0          iters = iters + 1\r\n",
      "    29                                                   \r\n",
      "    30                                                   #Updating v\r\n",
      "    31        17      20286.0   1193.3      0.0          zv = np.matmul(X.T,u0)\r\n",
      "    32        17        746.0     43.9      0.0          wv = abs(zv)**gamv     #data driven weight\r\n",
      "    33        17        851.0     50.1      0.0          sigsq = (SST - np.sum(zv**2))/(n*d-d)\r\n",
      "    34        17      17338.0   1019.9      0.0          tv = np.sort(np.append(abs(zv**wv),0))\r\n",
      "    35        17     574392.0  33787.8      0.0          pv = sum(tv>0)\r\n",
      "    36        17       1079.0     63.5      0.0          Bv = np.ones((d+1,1))*np.Inf\r\n",
      "    37    214642     700062.0      3.3      0.0          for i in range(0,pv):\r\n",
      "    38    214625     649291.0      3.0      0.0              lvc = tv[d-i]\r\n",
      "    39    214625   14171640.0     66.0      0.8              para = {'ttyp': ttypv, 'plambda': lvc/wv[wv!=0]}\r\n",
      "    40    214625   53304840.0    248.4      3.2              temp2 = thresh(zv[wv!=0],para['ttyp'],para['plambda'])\r\n",
      "    41    214625     579516.0      2.7      0.0              vc = temp2\r\n",
      "    42    214625 1552980078.0   7235.8     92.6              Bv[i] = sum(sum((X - u0[:,np.newaxis]*vc[:,np.newaxis].T)**2)/sigsq + i*np.log(n*d))\r\n",
      "    43        17        843.0     49.6      0.0          Iv = np.argmin(Bv)+1\r\n",
      "    44        17      17052.0   1003.1      0.0          temp = np.sort(np.append(abs(zv* wv),0))\r\n",
      "    45        17        107.0      6.3      0.0          lv = temp[d-Iv-1]\r\n",
      "    46        17       1238.0     72.8      0.0          para['plambda'] = np.multiply(lv, wv[wv!=0])\r\n",
      "    47        17       4161.0    244.8      0.0          temp2 = thresh(zv[wv!=0],para['ttyp'],para['plambda'])\r\n",
      "    48        17         38.0      2.2      0.0          v1 = temp2\r\n",
      "    49        17      44767.0   2633.4      0.0          v1 = v1/np.sqrt(sum(v1**2)) #v_new\r\n",
      "    50                                           \r\n",
      "    51                                                   #Updating u\r\n",
      "    52        17      10729.0    631.1      0.0          zu = np.matmul(X, v1)\r\n",
      "    53        17        172.0     10.1      0.0          wu = abs(zu)**gamu\r\n",
      "    54        17        410.0     24.1      0.0          sigsq = (SST - sum(zu**2))/(n*d-n)\r\n",
      "    55        17        927.0     54.5      0.0          tu = np.sort(np.append(abs(zu**wu),0))\r\n",
      "    56        17       2892.0    170.1      0.0          pu = sum(tu>0)\r\n",
      "    57        17        452.0     26.6      0.0          Bu = np.ones((n+1,1))*np.Inf\r\n",
      "    58       969       3049.0      3.1      0.0          for i in range(0,pu):\r\n",
      "    59       952       2992.0      3.1      0.0              luc = tu[n-i]\r\n",
      "    60       952      16263.0     17.1      0.0              para = {'ttyp': ttypu, 'plambda': luc/wu[wu!=0]}\r\n",
      "    61       952      40521.0     42.6      0.0              temp2 = thresh(zu[wu!=0],para['ttyp'],para['plambda'])\r\n",
      "    62       952       2189.0      2.3      0.0              uc = temp2\r\n",
      "    63       952    6880680.0   7227.6      0.4              Bu[i] = sum(sum((X - temp2[:,np.newaxis]*v1[:,np.newaxis].T)**2)/sigsq + i*np.log(n*d))\r\n",
      "    64        17        484.0     28.5      0.0          Iu = np.argmin(Bu)+1\r\n",
      "    65        17       1423.0     83.7      0.0          temp = np.sort(np.append(abs(zu*wu),0))\r\n",
      "    66        17         86.0      5.1      0.0          lu = temp[n-Iv-1]\r\n",
      "    67        17        263.0     15.5      0.0          para['plambda'] = lu/wu[wu!=0]\r\n",
      "    68        17        428.0     25.2      0.0          temp2 = thresh(zu[wu!=0],para['ttyp'],para['plambda'])\r\n",
      "    69        17         32.0      1.9      0.0          u1 = temp2\r\n",
      "    70        17        473.0     27.8      0.0          u1 = u1/np.sqrt(sum(u1**2)) #u_new\r\n",
      "    71                                           \r\n",
      "    72        17        790.0     46.5      0.0          ud = np.sqrt(np.sum((u0-u1)**2))\r\n",
      "    73        17       1125.0     66.2      0.0          vd = np.sqrt(np.sum((v0-v1)**2))\r\n",
      "    74        17         40.0      2.4      0.0          if iters > niter:\r\n",
      "    75                                                       print('Fail to converge! Increase the niter!')\r\n",
      "    76                                                       break\r\n",
      "    77        17         32.0      1.9      0.0          u0 = u1\r\n",
      "    78        17         35.0      2.1      0.0          v0 = v1\r\n",
      "    79         1          3.0      3.0      0.0      u = u1\r\n",
      "    80         1     107520.0 107520.0      0.0      v = v1\r\n",
      "    81         1         70.0     70.0      0.0      return u,v,iters"
     ]
    }
   ],
   "source": [
    "%cat ssvd_results.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, it is able to find that unoptimized vanilla ssvd function for lung cancer data takes 30 minutes for CPU times and 27 minutes for Wall time. It takes so long time to execute ssvd function when we consider the size of data. To reduce running time and optimize it with numba pacakage, we decide to profile this function by line. When we investigate the result of profiling, we could find that line 18, line 37~42, and line 58~63 consumes most of running time. These parts are related with singular value decomposition of matrix and updating singular vectors u and v. Thus, we decide to seperate them from original function and optimze them individually. For optimization, we use functions from Numba pacakage. First, we find that updating u and v includes iterative loop and we decide to optimize them with @njit and prange function which makes iteration even faster. Second, we optimized np.linalg.svd function with @jit decorator and this process had actually effect. Lastly, we replace while function in main ssvd function with prange so that running time for iterative steps can be reduced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True)\n",
    "def updateU(Bu,idx,n,d,pu,tu,wu,z,v1,X,sigsq):\n",
    "    for i in prange(0,pu):\n",
    "        luc = tu[idx[i]]\n",
    "        plambda = luc/wu[wu!=0]\n",
    "        tmp= np.sign(z[wu!=0])*(np.abs(z[wu!=0])>=plambda)  \n",
    "        uc = tmp*(np.abs(z[wu!=0])-plambda)\n",
    "        Bu[i] = np.sum((X - np.outer(uc,v1))**2)/sigsq + (i+1)*np.log(n*d) #this works best\n",
    "        \n",
    "    return Bu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True)\n",
    "def updateV(Bv,idx,n,d,pv,tv,wv,z,u0,X,sigsq):\n",
    "    for i in prange(0,pv):\n",
    "        lvc = tv[idx[i]]\n",
    "        plambda = lvc/wv[wv!=0]\n",
    "        tmp= np.sign(z[wv!=0])*(np.abs(z[wv!=0])>=plambda)                         \n",
    "        vc = tmp*(np.abs(z[wv!=0])-plambda)\n",
    "        Bv[i] = np.sum((X - np.outer(u0,vc))**2)/sigsq + (i+1)*np.log(n*d) #this works best\n",
    "        \n",
    "    return Bv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython = True)\n",
    "def svd(X):\n",
    "    return np.linalg.svd(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel = True)\n",
    "def ssvd_opt(X,param=None):\n",
    "    n, d = X.shape\n",
    "    ttypu = 1; ttypv = 1\n",
    "    gamu = 0; gamv = 0\n",
    "    u, s, v = svd(X)\n",
    "    v = v.T\n",
    "    u0 = u[:,0]; v0 = v[:,0]\n",
    "    tol = 1e-4\n",
    "    niter = 100\n",
    "    ud = 1;vd = 1\n",
    "    SST = np.sum(X**2)\n",
    "    for iters in prange(niter):\n",
    "        zv = X.T@u0\n",
    "        wv = np.abs(zv)**gamv\n",
    "        sigsq = (SST - np.sum(zv**2))/(n*d-d)\n",
    "        tv = np.sort(np.append(np.abs(zv**wv),0))\n",
    "        pv = np.sum(tv>0)\n",
    "        Bv = np.ones(d+1)*np.Inf\n",
    "        idx = np.arange(d-1,-1,-1)\n",
    "        Bv = updateV(Bv,idx,n,d,pv,tv,wv,zv,u0,X,sigsq)\n",
    "        Iv = np.argmin(Bv) + 1\n",
    "        lv = tv[d-Iv]\n",
    "        plambdav = lv/wv[wv!=0]\n",
    "        tmpv= np.sign(zv[wv!=0])*(np.abs(zv[wv!=0])>=plambdav)\n",
    "        v1 = tmpv*(np.abs(zv[wv!=0])-plambdav)   \n",
    "        v1 = v1/np.sqrt(np.sum(v1**2)) #v_new\n",
    "        zu = X@v1\n",
    "        wu = np.abs(zu)**gamu\n",
    "        sigsq = (SST - np.sum(zu**2))/(n*d-n)\n",
    "        tu = np.sort(np.append(np.abs(zu**wu),0))\n",
    "        pu = np.sum(tu>0)\n",
    "        Bu = np.ones(n+1)*np.Inf\n",
    "        idx = np.arange(n-1,-1,-1)\n",
    "        Bu = updateU(Bu,idx,n,d,pu,tu,wu,zu,v1,X,sigsq)\n",
    "        Iu = np.argmin(Bu) + 1\n",
    "        lu = tu[n-Iu]\n",
    "        plambdau = lu/wu[wu!=0]\n",
    "        tmpu= np.sign(zu[wu!=0])*(np.abs(zu[wu!=0])>=plambdau)  \n",
    "        u1 = tmpu*(np.abs(zu[wu!=0])-plambdau)    \n",
    "        u1 = u1/np.sqrt(np.sum(u1**2)) #u_new\n",
    "        ud = np.sqrt(np.sum((u0-u1)**2))\n",
    "        vd = np.sqrt(np.sum((v0-v1)**2)) \n",
    "        if ((vd<tol) and (ud<tol)):\n",
    "            break\n",
    "        elif iters == niter -1:\n",
    "            print('need to increase niter!')\n",
    "        u0 = u1\n",
    "        v0 = v1\n",
    "    u = u1\n",
    "    v = v1\n",
    "    return u,v,iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-49-53e52755b781>:14: NumbaPerformanceWarning: '@' is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))\n",
      "  zv = X.T@u0\n",
      "/opt/conda/lib/python3.6/site-packages/numba/typing/npydecl.py:958: NumbaPerformanceWarning: '@' is faster on contiguous arrays, called on (array(float64, 2d, C), array(float64, 1d, A))\n",
      "  warnings.warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "succeed to converge!\n",
      "CPU times: user 9min 52s, sys: 8min 3s, total: 17min 56s\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "u,v,iters = ssvd_opt(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "succeed to converge!\n",
      "CPU times: user 72 ms, sys: 164 ms, total: 236 ms\n",
      "Wall time: 16 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "us,vs,iterss = ssvd_opt(X_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing optimization process, it is able to find that the total CPU time reduced from 30 minutes to 17 miniutes which is much shorter time as much as half, and the Wall time is also reduced from about 27 minutes to 1 minutes 44 seconds. The optimized function is about 20 times faster than vanilla ssvd function for wall time. However, for simulated data, optimized function takes longer time than unoptimized function. We think this situation stems from preparation for parellalization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to simulation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same simulated data which is describe at the paper.  \n",
    "$X^* = suv^T$ with noise $\\epsilon$  \n",
    "where $u =  [10,9,8,7,6,5,4,3,r(2,17),r(0,75)]^T$, $v = [10,-10,8,-8,5,-5,r(-3,5),r(0,34)]^T$, s = 50  \n",
    "and u = u/||u||, v = v/||v||  \n",
    "where r(a,b) denotes that the vector whose length is b with all entries a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 50)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#simulation data - random sparse data\n",
    "np.random.seed(664)\n",
    "u_til = np.r_[np.arange(3,11)[::-1], 2*np.ones(17), np.zeros(75)].reshape(-1,1)\n",
    "u_til = u_til/np.linalg.norm(u_til)\n",
    "v_til = np.r_[np.array([10,-10,8,-8,5,-5]),3*np.ones(5),-3*np.ones(5),np.zeros(34)].reshape(-1,1)\n",
    "v_til = v_til/np.linalg.norm(v_til)\n",
    "s = 50\n",
    "X_sim = s*u_til@v_til.T\n",
    "n,p = X_sim.shape\n",
    "X_sim = X_sim + np.random.randn(n,p)\n",
    "X_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lung cancer data from paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breast cancer data from outside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative analysis with competing algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Lee et al. has done in their paper, we decide to compare other decompostion method using simulation data $X^* = suv^T$ with noise $\\epsilon$. The candidate models are regular SVD, Sparse PCA, and Spectral Biclustering from Klguer et al (2003). Sparse SVD is also finding latent sparse structure of data. Therefore, it is very similar algorithm with SSVD. However, SSVD and SPCA is definitely different in their purpose. The fundemental purpose of SPCA is reducing dimesion of data whereas the goal of SSVD is decomposing data and find latent association in data. Besides SVD based decomposition, we also compare biclustering method called Spectral Biclustering. The comparsion between these models are conducted based on their accuracy of finding actual latent stucture of data which is measured by the average number of 0 in decomposed vectors and their sensitivity and specificity in finding zero in decomposed vector. In addition to their accuracy, the running time for each algorithm are also examined. The existing function for svd is provided from Numpy.linalg function and for SPCA is provided from sklearn.decomposition package with SparsePCA function. Lastly, spectral biclustering from sklearn.cluster's SpectralBiclustering is also used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "from sklearn.cluster import SpectralBiclustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_compare(u,v,s,niter = 100):\n",
    "    X_temp = s*u*v.T\n",
    "    n,d = X_temp.shape\n",
    "    u_ssvd = np.zeros((len(u),niter));v_ssvd = np.zeros((len(v),niter));ssvd_time = np.zeros(niter) \n",
    "    u_svd = np.zeros((len(u),niter));v_svd = np.zeros((len(v),niter));svd_time = np.zeros(niter) \n",
    "    u_pca = np.zeros((len(u),niter));v_pca = np.zeros((len(v),niter));spca_time = np.zeros(niter)\n",
    "    u_bc = np.zeros((len(u),niter));v_bc = np.zeros((len(v),niter));bc_time = np.zeros(niter)\n",
    "    for i in range(niter):\n",
    "        X_sim = X_temp + np.random.randn(n,d)\n",
    "        ## ssvd\n",
    "        start = time.time()\n",
    "        u_ssvd[:,i],v_ssvd[:,i],iters = ssvd_opt(X_sim)\n",
    "        ssvd_time[i] = time.time() - start\n",
    "         \n",
    "        ## svd\n",
    "        start = time.time()\n",
    "        u1,s1,v1 = np.linalg.svd(X_sim)\n",
    "        u_svd[:,i],v_svd[:,i]= u1[:,0],v1[0,:]\n",
    "        svd_time[i] = time.time() - start\n",
    "        \n",
    "        ## spca\n",
    "        start = time.time()\n",
    "        model = decomposition.SparsePCA(n_components=1,alpha = 3)\n",
    "        model.fit(X_sim)\n",
    "        v_pca[:,i] = model.components_[0]\n",
    "        model.fit(X_sim.T)\n",
    "        u_pca[i,:] = model.components_[0]\n",
    "        spca_time[i] = time.time() - start\n",
    "    \n",
    "        ## SpectralBiclustering\n",
    "        start = time.time()\n",
    "        model = SpectralBiclustering(n_clusters=4,n_components=1,n_best = 1)\n",
    "        model.fit(X_sim)\n",
    "        u_bc[:,i] = model.row_labels_\n",
    "        v_bc[:,i] = model.column_labels_\n",
    "        bc_time[i] = time.time() - start\n",
    "        \n",
    "    time_table = np.c_[ssvd_time,svd_time,spca_time,bc_time]\n",
    "    time_table = time_table.mean(axis = 0)\n",
    "    \n",
    "    comparison_table = np.zeros((8,4))\n",
    "    row = 0\n",
    "    for uval in [u_ssvd,u_svd,u_pca,u_bc]:\n",
    "        comparison_table[2*row,:] = np.array(classify(uval,u)).mean(axis=1)\n",
    "        row = row+1\n",
    "    row = 0\n",
    "    for vval in [v_ssvd,v_svd,v_pca,v_bc]:\n",
    "        comparison_table[2*row+1,:] = np.array(classify(vval,v)).mean(axis=1)\n",
    "        row = row+1\n",
    "\n",
    "    return time_table, comparison_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_table, comparison_table = sim_compare(u_til,v_til,s=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_table = pd.DataFrame(np.round(time_table,decimals=3), index = [\"SSVD\",\"SVD\",\"SPCA\",\"SBC\"], columns=[\"Avg time by sec\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_table = pd.DataFrame(np.round(comparison_table,decimals=2),\n",
    "                                index = [\"SSVD-u\",\"SSVD-v\",\"SVD-u\",\"SVD-v\",\"SPCA-u\",\"SPCA-v\",\"SBC-u\",\"SBC-v\"],\n",
    "                                columns=[\"Avg # of 0\", \"Sensitivity of 0\",\"Specificity of 0\",\"Miss rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg # of 0</th>\n",
       "      <th>Sensitivity of 0</th>\n",
       "      <th>Specificity of 0</th>\n",
       "      <th>Miss rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SSVD-u</th>\n",
       "      <td>69.75</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SSVD-v</th>\n",
       "      <td>31.52</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD-u</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD-v</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPCA-u</th>\n",
       "      <td>75.54</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPCA-v</th>\n",
       "      <td>33.92</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SBC-u</th>\n",
       "      <td>53.36</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SBC-v</th>\n",
       "      <td>26.94</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Avg # of 0  Sensitivity of 0  Specificity of 0  Miss rate\n",
       "SSVD-u       69.75              0.93              1.00       0.05\n",
       "SSVD-v       31.52              0.93              1.00       0.05\n",
       "SVD-u         0.00              0.00              1.00       0.75\n",
       "SVD-v         0.00              0.00              1.00       0.68\n",
       "SPCA-u       75.54              0.75              0.24       0.37\n",
       "SPCA-v       33.92              1.00              1.00       0.00\n",
       "SBC-u        53.36              0.65              0.83       0.30\n",
       "SBC-v        26.94              0.67              0.74       0.31"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(comparison_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg time by sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SSVD</th>\n",
       "      <td>0.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD</th>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPCA</th>\n",
       "      <td>0.107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SBC</th>\n",
       "      <td>0.115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Avg time by sec\n",
       "SSVD            0.045\n",
       "SVD             0.003\n",
       "SPCA            0.107\n",
       "SBC             0.115"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(time_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we investigate the result of comparison between candidate models, it is able to find that SSVD overwhelm other decomposition and biclustering method in all of performances. First, SVD absolutely failed to capture underlying zero entries in singluar vectors. As a result, SVD captures all none-zero entries indicated as 100% specificity and 75%, 68% miss classification rate. Despite of poor classification performance, running time of SVD is much shorter than other methods. On the other hand, SPCA succeed to classify none-zero and zero entries in v vector. However, it shows poor performance in classifying zero and none-zero in u vector which overestimate the number of zero entries in u. Lastly, Spectral Biclustering shows relatively balanced results like SSVD. But the performance in sensitivity and specificity is much lower than SSVD. Moreover, the running time of SPCA and Spectral Biclustering is 2 times longer than SSVD. From this result, we can conclude that SSVD is better method than other methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion/conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this paper, we have rewritten Lee et al's R and matlab code into python code and optimized them to shorten their running time. This optimized Sparse Singular Value Decomposition function succeed to find latent association of both simulated data and real world data. As we have shown at comparison part, SSVD shows superiority over other methods in detecting latent sparse checkerboard structure in simulation data. Not only it shows better performance, but also its running time is shorter than other biclustering methods. This well-performed function can be applied to other gene micro-array data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References/bibliography"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
